{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import http.client\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "wd='C:/Users/tmehd/OneDrive/Desktop/AI/Project4/Reinforcement-Learning/'\n",
    "\n",
    "# Replace these with your own values\n",
    "api_key = \"13d493c2d91efe1c451a\"\n",
    "user_id = \"1171\"\n",
    "team_id = \"1343\"\n",
    "base_url = \"www.notexponential.com\"\n",
    "otp=5712768807\n",
    "\n",
    "headers = {\n",
    "  'x-api-key': '13d493c2d91efe1c451a', #change apikey\n",
    "  'userid': '1171', #change userid\n",
    "  'Content-Type': 'application/x-www-form-urlencoded',\n",
    "  'Authorization': 'Basic YWxpLmFzZ2Fyb3ZAZ3dtYWlsLmd3dS5lZHU6TWFydmVsMDc0QA=='\n",
    "}\n",
    "\n",
    "n_worlds = 11\n",
    "start_world=9\n",
    "end_world=10\n",
    "world_size = 40\n",
    "n_states = world_size * world_size\n",
    "n_actions = 4\n",
    "n_episodes = 30\n",
    "alpha = 0.8\n",
    "gamma = 0.99\n",
    "epsilon = 0.8\n",
    "\n",
    "# Initialize the Q-table\n",
    "# try:\n",
    "Q= np.load(f\"{wd}/Q.npy\")\n",
    "reward_map= np.load(f\"{wd}/reward_map.npy\")\n",
    "\n",
    "# except:\n",
    "#     Q = np.zeros((n_worlds, n_states, n_actions))\n",
    "\n",
    "\n",
    "# Reset Api\n",
    "def reset_api():\n",
    "    conn = http.client.HTTPSConnection(base_url)\n",
    "    payload = ''\n",
    "    conn.request(\"GET\", f\"/aip2pgaming/api/rl/reset.php?teamId={team_id}&otp={otp}\", payload, headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    print(data.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "# Define a function to send a request\n",
    "def send_request(method, path, headers, body=None):\n",
    "    conn = http.client.HTTPSConnection(base_url)\n",
    "    conn.request(method, path, body, headers)\n",
    "    response = conn.getresponse()\n",
    "    data = response.read().decode('utf-8')\n",
    "    conn.close()\n",
    "    return json.loads(data)\n",
    "\n",
    "# Define a function to enter a world\n",
    "def enter_world(world_id):\n",
    "    data = f\"type=enter&worldId={world_id}&teamId={team_id}\"\n",
    "    return send_request(\"POST\", \"/aip2pgaming/api/rl/gw.php\", headers, data)\n",
    "\n",
    "# Define a function to make a move\n",
    "def make_move(world_id, move):\n",
    "    if move==0:\n",
    "        move='N'\n",
    "    elif move==1:\n",
    "        move='S'\n",
    "    elif move==2:\n",
    "        move='E'\n",
    "    elif move==3:\n",
    "        move='W'\n",
    "        \n",
    "    data = f\"type=move&teamId={team_id}&move={move}&worldId={world_id}\"\n",
    "    return send_request(\"POST\", \"/aip2pgaming/api/rl/gw.php\", headers, data)\n",
    "# Define a function to visualize the grid world\n",
    "def plot_world(world_data):\n",
    "    plt.imshow(world_data, cmap='binary')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "# Main loop\n",
    "for episode in range(n_episodes):\n",
    "    print(episode, \" episode started\")\n",
    "    if episode!=0:\n",
    "        epsilon=epsilon*0.7\n",
    "        alpha=alpha*0.7\n",
    "    for world_id in range(start_world,end_world):\n",
    "        print(world_id, \" world started\")\n",
    "        reset_api()\n",
    "        enter_resp = enter_world(world_id)\n",
    "        score_map= np.load(f'{wd}/score_map_{world_id}')\n",
    "        world_data = np.zeros((world_size, world_size)) # ?\n",
    "\n",
    "        # Get the initial state\n",
    "        state_row, state_col = map(int, enter_resp[\"state\"].split(':'))\n",
    "        state = state_row * world_size + state_col\n",
    "        world_data[state_row, state_col] = 1\n",
    "        score_map[world_id,state_row, state_col]=1#?\n",
    "\n",
    "        for _ in range(world_size * world_size):\n",
    "            # Choose action using epsilon-greedy strategy\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                while True:\n",
    "                    a = random.randint(0, n_actions - 1)\n",
    "                    if Q[world_id, state, a]>=np.mean(Q[world_id, state]) or Q[world_id, state, a]==0:\n",
    "                        action=a\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                action = np.argmax(Q[world_id, state])\n",
    "\n",
    "            # Make a move and get the reward and next state\n",
    "            move_resp = make_move(world_id, action)\n",
    "            reward = float(move_resp[\"reward\"])\n",
    "            reward_map[world_id]=reward_map[world_id]+reward\n",
    "\n",
    "            # Get the next state\n",
    "            try:\n",
    "                next_state_row, next_state_col = int(move_resp[\"newState\"][\"x\"]), int(move_resp[\"newState\"][\"y\"])\n",
    "                next_state = next_state_row * world_size + next_state_col\n",
    "                world_data[next_state_row, next_state_col] = 1\n",
    "                score_map[world_id,next_state_row, next_state_col]=1\n",
    "            except:\n",
    "                # If there is an error, the game is over\n",
    "                Q[world_id, state, action] += alpha * (reward - Q[world_id, state, action])\n",
    "                \n",
    "                # Q[world_id, state, action] = Q[world_id, state, action]+ alpha * (reward - Q[world_id, state, action])\n",
    "                \n",
    "                np.save(f'{wd}/Q.npy', Q)\n",
    "                np.save(f'{wd}/score_map_{world_id}', score_map)\n",
    "                np.save('C:{wd}/reward_map.npy', reward_map)\n",
    "                if reward<0:\n",
    "                    score_map[world_id,state_row, state_col]=2\n",
    "                else:\n",
    "                    score_map[world_id,state_row, state_col]=3\n",
    "                break\n",
    "\n",
    "            # Update Q-table\n",
    "            Q[world_id, state, action] += alpha * (reward + gamma * np.max(Q[world_id, next_state]) - Q[world_id, state, action])\n",
    "            np.save(f'{wd}/Q.npy', Q)\n",
    "            np.save(f'{wd}/score_map_{world_id}', score_map)\n",
    "            np.save(f'{wd}/reward_map.npy', reward_map)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            # Visualize the grid world\n",
    "            # plot_world(world_data)\n",
    "            print(\"Reward:\", reward)\n",
    "            print(\"Reward:\", reward)\n",
    "\n",
    "\n",
    "            # Introduce a delay before the next action\n",
    "            time.sleep(1)\n",
    "\n",
    "        \n",
    "        print(world_id, \" ended\")\n",
    "        # Introduce a delay before entering the next world\n",
    "#         time.sleep(10 * 60)\n",
    "        \n",
    "\n",
    "# Print the learned policy\n",
    "for world_id in range(n_worlds):\n",
    "    print(f\"World {world_id}:\")\n",
    "    for state in range(n_states):\n",
    "        print(f\"  In state {state}, take action: {np.argmax(Q[world_id, state])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
